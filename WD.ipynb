{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlessandroPata/CNN_Watermark_Detection/blob/main/WD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPqueqDn8SrV"
      },
      "outputs": [],
      "source": [
        "#Dataset Upload and pre-processing\n",
        "#Consider to execute the code using a 300gb Ram TPU (TPU v2) at least!\n",
        "#The whole project can be consulted on Github. https://github.com/AlessandroPata/CNN_Watermark_Detection\n",
        "!pip install gdown\n",
        "import gdown\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "!pip install tensorflow\n",
        "!pip install keras-tuner\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from kerastuner import HyperModel, RandomSearch\n",
        "!pip install kaggle\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import gc\n",
        "!kaggle datasets download -d alessandropata/watermark-det\n",
        "!unzip watermark-det.zip\n",
        "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/Watermark_Detection_ver_2.0\",\n",
        "    image_size=(224, 224),\n",
        "    batch_size=1,\n",
        "    label_mode='int'\n",
        ")\n",
        "dataset=dataset.shuffle(buffer_size=70000)\n",
        "def filter_and_shuffle(ds, label, shuffle_buffer_size=70000):\n",
        "    return ds.filter(lambda x, y: tf.math.equal(y[0], label)).shuffle(buffer_size=shuffle_buffer_size)\n",
        "nw_dataset = filter_and_shuffle(dataset, 0)\n",
        "w_dataset = filter_and_shuffle(dataset, 1)\n",
        "train_nw=nw_dataset.take(24184)\n",
        "train_w=w_dataset.take(24184)\n",
        "valtestnw=nw_dataset.skip(24184).take(12739)\n",
        "valtestw=w_dataset.skip(24184).take(7992)\n",
        "train_dataset = train_nw.concatenate(train_w).shuffle(buffer_size=70000)\n",
        "valtest = valtestnw.concatenate(valtestw).shuffle(buffer_size=70000)\n",
        "val_dataset=valtest.take(10365)\n",
        "test_dataset=valtest.skip(10365).take(10365)\n",
        "batch_size = 32\n",
        "train_dataset = train_dataset.unbatch().batch(batch_size)\n",
        "val_dataset= val_dataset.unbatch().batch(batch_size)\n",
        "test_dataset= test_dataset.unbatch().batch(batch_size)\n",
        "train = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val = val_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test = test_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "!rm -r ./sample_data\n",
        "!rm -r ./watermark-det.zip\n",
        "test_labels = []\n",
        "for _, label_batch in test:\n",
        "    test_labels.extend(label_batch.numpy())\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBBy4EpfX7xe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#Model Configuration and Testing\n",
        "#The lines of code included in the comment were useful for finding the most performant model via Keras Tuner and for training the hyperparameters.\n",
        "#There is no need to execute them, as the trained model can be downloaded from Google Drive.\n",
        "class BinaryClassifierHyperModel(HyperModel):\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = Sequential([\n",
        "            Conv2D(\n",
        "                filters = hp.Choice('conv_1_filter', values=[64]),\n",
        "                kernel_size=hp.Choice('conv_1_kernel', values=[5]),\n",
        "                activation='relu',\n",
        "                input_shape=self.input_shape,\n",
        "                kernel_regularizer=l2(hp.Float('conv_1_l2', min_value=6e-5, max_value=10e-5, sampling='log'))\n",
        "            ),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            Conv2D(\n",
        "                filters=hp.Choice('conv_2_filter', values=[64]),\n",
        "                kernel_size=hp.Choice('conv_2_kernel', values=[3]),\n",
        "                activation='relu',\n",
        "                kernel_regularizer=l2(hp.Float('conv_2_l2', min_value=2e-5, max_value=8e-5, sampling='log'))\n",
        "            ),\n",
        "            BatchNormalization(),\n",
        "            MaxPooling2D(pool_size=(2, 2)),\n",
        "            Flatten(),\n",
        "            Dense(\n",
        "                units=hp.Choice('dense_units', values=[512]),\n",
        "                activation='relu',\n",
        "                kernel_regularizer=l2(hp.Float('dense_l2', min_value=4e-5, max_value=8e-4, sampling='log'))\n",
        "            ),\n",
        "            Dropout(0.5),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=Adam(hp.Choice('learning_rate', values=[1e-4])),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "hypermodel = BinaryClassifierHyperModel(input_shape=(224, 224, 3))\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=64,\n",
        "    executions_per_trial=1,\n",
        "    directory='/content/drive/MyDrive/Watermark/tuner/',\n",
        "    project_name='hparam_tuning'\n",
        ")\n",
        "tuner.search(train, epochs=9, validation_data=val, callbacks=callbacks)\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=5,\n",
        "        activation='relu',\n",
        "        input_shape=(224, 224, 3),\n",
        "        kernel_regularizer=l2(8.916867698179726e-05)\n",
        "    ),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=3,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=l2(2.785257633027672e-05)\n",
        "    ),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(\n",
        "        units=512,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=l2(0.0006143210398780726)\n",
        "    ),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\"\"\"\n",
        "callbacks = [\n",
        "    ModelCheckpoint('/content/Watermark/model.keras', save_best_only=True,save_weights_only=False, monitor='val_loss', mode='min'),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=0),\n",
        "    EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
        "]\n",
        "# Download the model file from Google Drive\n",
        "url = 'https://drive.google.com/uc?export=download&id=1-6eUOTLs4X_6-nMUyj-Kc5gFWi64V2yz'\n",
        "output_file = 'model.keras'\n",
        "gdown.download(url, output_file, quiet=False)\n",
        "# Load the model from the downloaded file\n",
        "model = load_model('model.keras')\n",
        "\"\"\"\n",
        "training = model.fit(\n",
        "    train,\n",
        "    epochs=10000,\n",
        "    validation_data=val,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\"\"\"\n",
        "test_loss, test_acc = model.evaluate(test)\n",
        "#pd.DataFrame(training.history)[[\"accuracy\", \"val_accuracy\"]].plot()\n",
        "test_predictions = model.predict(test).round().astype(int)\n",
        "test_predictions = test_predictions.flatten()\n",
        "cm = confusion_matrix(test_labels, test_predictions)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "print(sklearn.metrics.classification_report(test_labels, test_predictions, labels=[0,1],target_names=[\"no_watermark\",\"watermark\"], digits=4))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27SbWzq2wo7S"
      },
      "outputs": [],
      "source": [
        "#Printing Missclassified Images\n",
        "misclassified_images = []\n",
        "misclassified_labels = []\n",
        "misclassified_predictions = []\n",
        "index = 0\n",
        "for images, labels in test:\n",
        "    for j in range(images.shape[0]):\n",
        "        if test_predictions[index] =! labels[j].numpy():\n",
        "            misclassified_images.append(images[j].numpy().astype(\"uint8\"))\n",
        "            misclassified_labels.append(labels[j].numpy())\n",
        "            misclassified_predictions.append(test_predictions[index])\n",
        "        index += 1\n",
        "\n",
        "# Iterate through misclassified images and display them\n",
        "for i in range(len(misclassified_images)):\n",
        "    plt.figure()\n",
        "    plt.imshow(misclassified_images[i])\n",
        "    plt.title(f\"True label: {misclassified_labels[i]}, Predicted: {misclassified_predictions[i]}\")\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM/Q20O1I6NRjjict27E/BO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}